{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: .\\config.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/prep.py \n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "# Check if given credential can get token successfully.\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "secret_client = SecretClient(vault_url=\"https://mlgroup.vault.azure.net/\", credential=credential)\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--blob_storage\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--prep_output\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# log in to the Blob Service Client\n",
    "blob_storage = args.blob_storage\n",
    "blob_storage_key = secret_client.get_secret(\"blob-storage-key\")\n",
    "blob_service_client = BlobServiceClient(blob_storage, account_key=blob_storage_key.value)\n",
    "\n",
    "# connect to the container \n",
    "container_client = blob_service_client.get_container_client(container=\"stock-news-json\") \n",
    "\n",
    "# list and download all currently available blobs\n",
    "blob_list = container_client.list_blobs()\n",
    "print(f\"Blob from: {blob_storage} has these blobs today: {blob_list}\")\n",
    "\n",
    "# get the timestamp with the current day \n",
    "current_day_date = datetime.datetime.today().isoformat()[:10]\n",
    "\n",
    "# filter out which blobs have the current date and download them\n",
    "blobs_to_use = [blob.name for blob in blob_list if current_day_date in blob.name]\n",
    "for blob in blobs_to_use:\n",
    "      print(f\"Downloading blob: {blob}\")\n",
    "      blob_client = blob_service_client.get_blob_client(container=\"stock-news-json\", blob=blob)\n",
    "      with open(blob, mode=\"wb\") as sample_blob:\n",
    "            download_stream = blob_client.download_blob()\n",
    "            sample_blob.write(download_stream.readall())\n",
    "\n",
    "# combine all blobs into one dictionary\n",
    "all_data_dict = {}\n",
    "for json_file in blobs_to_use:\n",
    "      with open(json_file,\"r+\") as file:\n",
    "      # First we load existing data into a dict.\n",
    "            file_data = json.load(file)\n",
    "            all_data_dict.update(file_data)\n",
    "\n",
    "# pass aggregated file to the next step        \n",
    "with open((Path(args.prep_output) / \"merged_stock_news.json\"), \"w\") as file:\n",
    "      file.write(json.dumps(all_data_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/classify.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Define constants for model names\n",
    "DESTILBERT_MODEL_NAME = \"KernAI/stock-news-destilbert\"\n",
    "FINBERT_MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "# Define a dictionary to map model names to their tokenizers\n",
    "MODEL_NAME_TO_TOKENIZER = {\n",
    "    DESTILBERT_MODEL_NAME: AutoTokenizer,\n",
    "    FINBERT_MODEL_NAME: AutoTokenizer,\n",
    "}\n",
    "\n",
    "# Define a dictionary to map model names to their models\n",
    "MODEL_NAME_TO_MODEL = {\n",
    "    DESTILBERT_MODEL_NAME: AutoModelForSequenceClassification,\n",
    "    FINBERT_MODEL_NAME: AutoModelForSequenceClassification,\n",
    "}\n",
    "\n",
    "def download_model(model_name: str):\n",
    "    model = MODEL_NAME_TO_MODEL[model_name].from_pretrained(model_name)\n",
    "    tokenizer = MODEL_NAME_TO_TOKENIZER[model_name].from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def use_model(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    text: str\n",
    "    ) -> str:\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        is_split_into_words=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    outputs = model(**tokenized_text)\n",
    "    outputs_logits = outputs.logits.argmax(1)\n",
    "\n",
    "    if isinstance(model, transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification):\n",
    "        mapping = {0: 'neutral', 1: 'negative', 2: 'positive'} # distilbert mapping\n",
    "    else:\n",
    "        mapping = {0: 'positive', 1: 'negative', 2: 'neutral'} # finbert mapping\n",
    "\n",
    "    return mapping[int(outputs_logits[0])]\n",
    "\n",
    "# Parse command-line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--classify_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--classify_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Download models\n",
    "destilbert_model, destilbert_tokenizer = download_model(DESTILBERT_MODEL_NAME)\n",
    "finbert_model, finbert_tokenizer = download_model(FINBERT_MODEL_NAME)\n",
    "\n",
    "# Read input data\n",
    "input_file_path = os.path.join(args.classify_input, \"merged_stock_news.json\")\n",
    "with open(input_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Iterate through tickers\n",
    "for ticker, ticker_data in data.items():\n",
    "    texts = ticker_data[\"texts\"]\n",
    "\n",
    "    # Use the models and append sentiments\n",
    "    sentiments_distilbert = [use_model(destilbert_model, destilbert_tokenizer, text) for text in texts]\n",
    "    sentiments_finbert = [use_model(finbert_model, finbert_tokenizer, text) for text in texts]\n",
    "\n",
    "    # Update the data with sentiments\n",
    "    ticker_data[\"sentiments\"] = sentiments_distilbert\n",
    "    ticker_data[\"sentiments_finbert\"] = sentiments_finbert\n",
    "\n",
    "# Write the updated data back to the output file\n",
    "output_file_path = Path(args.classify_output) / \"merged_stock_news.json\"\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/summarize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/summarize.py\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime \n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "# check if given credential can get token successfully.\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "secret_client = SecretClient(vault_url=\"https://mlgroup.vault.azure.net/\", credential=credential)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--summarize_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--summarize_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# retriev the list of blobs from the current day - input is a .txt file\n",
    "with open(os.path.join(args.summarize_input, \"merged_stock_news.json\"), \"r\") as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "# authenticate to openai\n",
    "api_key = api_key=secret_client.get_secret(\"openai-key\")\n",
    "openai_client = OpenAI(api_key=api_key.value)\n",
    "\n",
    "# get a list of all tickers, summaries all texts for each ticker\n",
    "tickers = list(data.keys())\n",
    "for ticker in tickers:\n",
    "    texts = data[ticker][\"texts\"]\n",
    "\n",
    "    summaries = []\n",
    "    for text in texts: \n",
    "        response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                    You are a helpful assistant for summarizing stock and finance news. \n",
    "                    Ensure to include numbers like stock price changes or concrete earning numbers.\n",
    "                    Keep it as short as possible.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please summarize the following article in one short sentence: {text}\"},\n",
    "            ],\n",
    "            max_tokens=60, \n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        summaries.append(response.choices[0].message.content)\n",
    "\n",
    "    # add the sentiments to the data\n",
    "    data[ticker][\"summaries\"] = summaries\n",
    "\n",
    "# connect and authenticate to the blob client\n",
    "account_url = \"https://mlstorageleo.blob.core.windows.net\"\n",
    "file_name = f\"processed-stock-news-{datetime.datetime.today().isoformat()[:10]}.json\"\n",
    "\n",
    "# create the BlobServiceClient object\n",
    "blob_data = json.dumps(data)\n",
    "blob_storage_key = secret_client.get_secret(\"blob-storage-key\")\n",
    "blob_service_client = BlobServiceClient(account_url, credential=blob_storage_key.value)\n",
    "blob_client = blob_service_client.get_blob_client(container=\"processed-stock-news-json\", blob=file_name)\n",
    "blob_client.upload_blob(blob_data)\n",
    "\n",
    "# overwrite old files with new files containing the sentiment\n",
    "with open((Path(args.summarize_output) / \"merged_stock_news.json\"), \"w\") as f:\n",
    "      json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/notify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/notify.py\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime \n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.communication.email import EmailClient\n",
    "\n",
    "# Define constants for the email subject and sender address\n",
    "EMAIL_SUBJECT = f\"Stock news analysis for {datetime.datetime.today().isoformat()[:10]}\"\n",
    "SENDER_ADDRESS = \"DoNotReply@632a8f5c-5cc8-4c44-8e7e-f509c76d0d24.azurecomm.net\"\n",
    "RECIPIENT_ADDRESS = \"leopuettmann@outlook.de\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--notify_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--notify_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "with open(os.path.join(args.notify_input, \"merged_stock_news.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data)\n",
    "# print(type(data))\n",
    "\n",
    "def format_data(ticker):\n",
    "    global data\n",
    "    summaries = \" \".join(data[ticker][\"summaries\"])\n",
    "    sentiments = (\n",
    "        data[ticker][\"sentiments\"].count(\"positive\"),\n",
    "        data[ticker][\"sentiments\"].count(\"neutral\"),\n",
    "        data[ticker][\"sentiments\"].count(\"negative\"),\n",
    "    )\n",
    "    texts = \" \".join(data[ticker][\"url\"])\n",
    "    return summaries, sentiments, texts\n",
    "\n",
    "msft_summaries, msft_sentiments, msft_texts = format_data(\"MSFT\")\n",
    "docn_summaries, docn_sentiments, docn_texts = format_data(\"DOCN\")\n",
    "txn_summaries, txn_sentiments, txn_texts = format_data(\"TXN\")\n",
    "\n",
    "email_content = f\"\"\"\n",
    "This is your daily stock news summary. \n",
    "\n",
    "===\n",
    "News about Microsoft: \n",
    "{msft_summaries}\n",
    "\\n Sentiments: positive -> {msft_sentiments[0]} | neutral -> {msft_sentiments[1]} | negative -> {msft_sentiments[2]}\n",
    "=== \\n\\n\n",
    "\n",
    "===\n",
    "News about DigitalOcean: \n",
    "{docn_summaries}\n",
    "\\n Sentiments: positive -> {docn_sentiments[0]} | neutral -> {docn_sentiments[1]} | negative -> {docn_sentiments[2]}\n",
    "=== \\n\\n\n",
    "\n",
    "===\n",
    "News about Texas Instruments: \n",
    "{txn_summaries}\n",
    "\\n Sentiments: positive -> {txn_sentiments[0]} | neutral -> {txn_sentiments[1]} | negative -> {txn_sentiments[2]}\n",
    "===\n",
    "\"\"\"\n",
    "\n",
    "def send_email(email_client, subject, content, recipient, sender):\n",
    "    message = {\n",
    "        \"content\": {\n",
    "            \"subject\": subject,\n",
    "            \"plainText\": content\n",
    "        },\n",
    "        \"recipients\": {\n",
    "            \"to\": [\n",
    "                {\n",
    "                    \"address\": recipient,\n",
    "                    \"displayName\": \"Leo\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"senderAddress\": sender\n",
    "    }\n",
    "    poller = email_client.begin_send(message)\n",
    "\n",
    "# Initialize Azure services and clients\n",
    "credential = DefaultAzureCredential()\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "secret_client = SecretClient(vault_url=\"https://mlgroup.vault.azure.net/\", credential=credential)\n",
    "connection_string = secret_client.get_secret(\"mail-connection-string\")\n",
    "email_client = EmailClient.from_connection_string(connection_string.value)\n",
    "\n",
    "# Send email\n",
    "send_email(email_client, EMAIL_SUBJECT, email_content, RECIPIENT_ADDRESS, SENDER_ADDRESS)\n",
    "\n",
    "# Pass merged stock news JSON file to the output of the pipeline\n",
    "with open((Path(args.notify_output) / \"merged_stock_news.json\"), \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: stock-analysis-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - azure-storage-blob\n",
    "    - azure-identity\n",
    "    - azure-keyvault\n",
    "    - azure-communication-email\n",
    "    - transformers\n",
    "    - torch\n",
    "    - sentencepiece\n",
    "    - numpy\n",
    "    - openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info for the env \n",
    "custom_env_name = \"stock-analysis-env\"\n",
    "version = \"1.8\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=version)\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock analysis pipeline\",\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/python-sdk-v2:4\",\n",
    "        version=version,\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for the compontents\n",
    "data_type = AssetTypes.URI_FOLDER\n",
    "path = \"azureml://datastores/stocknewsjson/stock-news-json\"\n",
    "input_mode = InputOutputModes.RO_MOUNT\n",
    "output_mode = InputOutputModes.RW_MOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Finding out which blobs to actually use\",\n",
    "    description=\"Loads files from Azure Blob Storage from todays \",\n",
    "    inputs={\n",
    "        \"blob_storage\": Input(mode=InputOutputModes.DIRECT)\n",
    "    },\n",
    "    outputs={\n",
    "        \"prep_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/prep.py\",\n",
    "    command=\"python prep.py --blob_storage ${{inputs.blob_storage}} --prep_output ${{outputs.prep_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_component = command(\n",
    "    name=\"classify\",\n",
    "    display_name=\"Classify the sentiments of todays stock news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"classify_input\": Input(type=data_type, mode=input_mode), \n",
    "    },\n",
    "    outputs={\n",
    "        \"classify_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/classify.py\",\n",
    "    command=\"python classify.py --classify_input ${{inputs.classify_input}} --classify_output ${{outputs.classify_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_component = command(\n",
    "    name=\"summarize\",\n",
    "    display_name=\"Summarize the news\",\n",
    "    description=\"Uses a pegasus model to summarize the news aricle\",\n",
    "    inputs={\n",
    "        \"summarize_input\": Input(type=data_type, mode=input_mode),\n",
    "    },\n",
    "    outputs={\n",
    "        \"summarize_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/summarize.py\",\n",
    "    command=\"python summarize.py --summarize_input ${{inputs.summarize_input}} --summarize_output ${{outputs.summarize_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify_component = command(\n",
    "    name=\"notify\",\n",
    "    display_name=\"Notify the user via Mail\",\n",
    "    description=\"Sends out an E-Mail with the results of the pipeline\",\n",
    "    inputs={\n",
    "        \"notify_input\": Input(type=data_type, mode=input_mode),\n",
    "    },\n",
    "    outputs={\n",
    "        \"notify_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/notify.py\",\n",
    "    command=\"python notify.py --notify_input ${{inputs.notify_input}} --notify_output ${{outputs.notify_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline(compute=\"ava\")\n",
    "def stock_news_pipeline():\n",
    "\n",
    "    data_prep_job = data_prep_component(\n",
    "        blob_storage=\"https://mlstorageleo.blob.core.windows.net/\"\n",
    "    )\n",
    "    classify_job = classify_component(\n",
    "        classify_input=data_prep_job.outputs.prep_output\n",
    "\n",
    "    ) \n",
    "    summarize_job = summarize_component(\n",
    "        summarize_input = classify_job.outputs.classify_output\n",
    "    )\n",
    "\n",
    "    notify_job = notify_component(\n",
    "        notify_input = summarize_job.outputs.summarize_output\n",
    "    )\n",
    "\n",
    "    return {\"processed_file\": notify_job.outputs.notify_output}\n",
    "\n",
    "pipeline_job = stock_news_pipeline()\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"ava\"\n",
    "pipeline_job.settings.reuse_component = \"false\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading notify.py\u001b[32m (< 1 MB): 100%|##########| 3.22k/3.22k [00:00<00:00, 9.17kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>stock-news-analysis-pipeline</td><td>placid_clock_hqy07ynrl9</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/placid_clock_hqy07ynrl9?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace&amp;tid=08548f02-0216-4325-938b-fd30f6829e55\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {'processed_file': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x000002A429D8E160>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\azure-stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002A4288B1FA0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'stock_news_pipeline', 'is_deterministic': None, 'inputs': {}, 'outputs': {'processed_file': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'data_prep_job': Command({'parameters': {}, 'init': False, 'name': 'data_prep_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\azure-stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002A42B1C1820>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Finding out which blobs to actually use', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'blob_storage': {'type': 'uri_folder', 'path': 'https://mlstorageleo.blob.core.windows.net/'}}, 'job_outputs': {'prep_output': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'blob_storage': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002A42B1C1790>}, 'outputs': {'prep_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002A42B1C1E20>}, 'component': 'azureml_anonymous:2ef78780-8301-413d-8ed8-92a0c8cdce60', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '2f018ad9-e776-4e82-90c4-44487587936d', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'classify_job': Command({'parameters': {}, 'init': False, 'name': 'classify_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\azure-stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002A428B04D60>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Classify the sentiments of todays stock news', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'classify_input': '${{parent.jobs.data_prep_job.outputs.prep_output}}'}, 'job_outputs': {'classify_output': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'classify_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002A428B04100>}, 'outputs': {'classify_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002A428B04BE0>}, 'component': 'azureml_anonymous:6202dfb1-e3f1-490e-89bb-48c642c8c3ea', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'e93f1fd1-a535-44c7-833b-0552a91c412e', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'summarize_job': Command({'parameters': {}, 'init': False, 'name': 'summarize_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\azure-stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002A4288B1D60>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Summarize the news', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'summarize_input': '${{parent.jobs.classify_job.outputs.classify_output}}'}, 'job_outputs': {'summarize_output': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'summarize_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002A4288B1D00>}, 'outputs': {'summarize_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002A4288B1CD0>}, 'component': 'azureml_anonymous:783be30b-8e4b-42cc-9867-b0db93ddd0ae', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '1bd4a8d8-d601-4029-8037-c91203f6661e', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'notify_job': Command({'parameters': {}, 'init': False, 'name': 'notify_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\azure-stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002A4288B1DC0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Notify the user via Mail', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'notify_input': '${{parent.jobs.summarize_job.outputs.summarize_output}}'}, 'job_outputs': {'notify_output': '${{parent.outputs.processed_file}}'}, 'inputs': {'notify_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002A4288B1F40>}, 'outputs': {'notify_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002A4288B1E80>}, 'component': 'azureml_anonymous:466273e5-60db-4969-be44-ad6a089202e4', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '42f87d4f-3334-48e0-bac5-c2323160df85', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 4}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 4}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'placid_clock_hqy07ynrl9', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'git@github.com:LeonardPuettmann/azure-stock-news-analysis.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'a56f44d124b461e01c7085d2f2611e6e11af9733', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'ava', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/MlGroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace/jobs/placid_clock_hqy07ynrl9', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\azure-stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000002A429D8E0A0>, 'serialize': <msrest.serialization.Serializer object at 0x000002A429D8E1C0>, 'display_name': 'stock_news_pipeline', 'experiment_name': 'stock-news-analysis-pipeline', 'compute': 'ava', 'services': {'Tracking': {'endpoint': 'azureml://northeurope.api.azureml.ms/mlflow/v1.0/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/MlGroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/placid_clock_hqy07ynrl9?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace&tid=08548f02-0216-4325-938b-fd30f6829e55', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"stock-news-analysis-pipeline\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: placid_clock_hqy07ynrl9\n",
      "Web View: https://ml.azure.com/runs/placid_clock_hqy07ynrl9?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-12-05 23:10:11Z] Submitting 1 runs, first five are: 6f33ac92:ab8ca861-ca44-4a46-b816-5192858c8737\n",
      "[2023-12-05 23:11:09Z] Completing processing run id ab8ca861-ca44-4a46-b816-5192858c8737.\n",
      "[2023-12-05 23:11:09Z] Submitting 1 runs, first five are: afa21e37:f0329c91-c704-426a-a6ee-e11cd981f03c\n",
      "[2023-12-05 23:13:08Z] Completing processing run id f0329c91-c704-426a-a6ee-e11cd981f03c.\n",
      "[2023-12-05 23:13:08Z] Submitting 1 runs, first five are: de6f8640:ae9ac3d9-e113-4290-8aa9-b2b994110d03\n",
      "[2023-12-05 23:26:56Z] Completing processing run id ae9ac3d9-e113-4290-8aa9-b2b994110d03.\n",
      "[2023-12-05 23:26:56Z] Submitting 1 runs, first five are: 7141de1e:856e722b-8a6f-490e-a27f-a29000f8cdb8\n",
      "[2023-12-05 23:27:48Z] Completing processing run id 856e722b-8a6f-490e-a27f-a29000f8cdb8.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: placid_clock_hqy07ynrl9\n",
      "Web View: https://ml.azure.com/runs/placid_clock_hqy07ynrl9?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
