{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import constants\n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=constants.SUBSCRIPTION_ID,\n",
    "    resource_group_name=constants.RESOURCE_GROUP_NAME,\n",
    "    workspace_name=constants.WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/prep.py \n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(\"prep\")\n",
    "    parser.add_argument(\"--input_data\", type=str, help=\"Path of prepped data\")\n",
    "    parser.add_argument(\"--output_data\", type=str, help=\"Path of prepped data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # log in to the Blob Service Client\n",
    "    account_url = \"https://mlstorageleo.blob.core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient(account_url, account_key=constants.BLOB_KEY)\n",
    "\n",
    "    # connect to the container \n",
    "    container_client = blob_service_client.get_container_client(container=\"stock-news-json\") \n",
    "\n",
    "    # list and download all currently available blobs\n",
    "    blob_list = container_client.list_blobs()\n",
    "\n",
    "    # get the timestamp with the current day \n",
    "    current_day_timestamp = datetime.datetime.today().timestamp()\n",
    "    current_day_timestamp = str(current_day_timestamp)[:8] # first 8 digits are the timestamp of the day\n",
    "\n",
    "    blobs_to_use = [blob.name for blob in blob_list if current_day_timestamp in blob.name]\n",
    "\n",
    "    # ! the files should not be downloaded in this step. Instead it might make more sense to pass a list with the filenames to the next component\n",
    "    with open(str(args.output_data)+\"blobs_to_use.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(blob for blob in blobs_to_use), f)\n",
    "    \n",
    "    \n",
    "    # for blob in blobs_to_download:\n",
    "    #     download_file_path = os.path.join(args.prep_data, str(blob))\n",
    "    #     with open(file=download_file_path, mode=\"wb\") as download_file:\n",
    "    #         download_file.write(container_client.download_blob(blob).readall())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/classify.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"path or URL to input data\")\n",
    "parser.add_argument(\"--folder_path\", type=str, help=\"path or URL to output data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# download distilbert model from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "\n",
    "def main():\n",
    "      # retriev the list of blobs from the current day - input is a .txt file\n",
    "      with open(args.input_data, \"r\") as f:\n",
    "            blobs_to_use = f.read()\n",
    "\n",
    "\n",
    "      dir_list = args.folder_path\n",
    "      for file_name in [file for file in os.listdir(dir_list) if file in blobs_to_use]:\n",
    "            with open(dir_list + file_name) as json_file:\n",
    "                  data = json.load(json_file)\n",
    "            texts = data[\"texts\"]\n",
    "\n",
    "            sentiments = []\n",
    "            for text in texts: \n",
    "                  tokenized_text = tokenizer(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        is_split_into_words=False,\n",
    "                        return_tensors=\"pt\"\n",
    "                  )\n",
    "\n",
    "                  outputs = model(tokenized_text[\"input_ids\"])\n",
    "                  outputs_logits = outputs.logits.argmax(1)\n",
    "\n",
    "                  mapping = {0: 'neutral', 1: 'negative', 2: 'positive'}\n",
    "                  predicted_label = mapping[int(outputs_logits[0])]\n",
    "                  sentiments.append(predicted_label)\n",
    "\n",
    "            # add the sentiments to the data\n",
    "            data[\"sentiments\"] = sentiments\n",
    "\n",
    "            # overwrite old files with new files containing the sentiment\n",
    "            with open(dir_list+file_name, \"w\") as f:\n",
    "                  json.dump(data, f)\n",
    "\n",
    "            # Note: no dedicated output needed here: we'll take the output from the first component again for the next step\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/summarize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/summarize.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"path or URL to input data\")\n",
    "parser.add_argument(\"--folder_path\", type=str, help=\"path or URL to output data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the model and the tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\") \n",
    "\n",
    "def main():\n",
    "      #dir_list = os.listdir(args.input_data)\n",
    "      dir_list = args.input_data\n",
    "      for file_name in [file for file in os.listdir(dir_list) if file.endswith('.json')]:\n",
    "            with open(dir_list + file_name) as json_file:\n",
    "                  data = json.load(json_file)\n",
    "            texts = data[\"texts\"]\n",
    "\n",
    "            summaries = []\n",
    "            for text in texts: \n",
    "                # Tokenize our text\n",
    "                # If you want to run the code in Tensorflow, please remember to return the particular tensors as simply as using return_tensors = 'tf'\n",
    "                input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "                # Generate the output (Here, we use beam search but you can also use any other strategy you like)\n",
    "                output = model.generate(\n",
    "                    input_ids, \n",
    "                    max_length=32, \n",
    "                    num_beams=5, \n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "                # Finally, we can print the generated summary\n",
    "                summaries.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "            # add the sentiments to the data\n",
    "            data[\"summaries\"] = summaries\n",
    "\n",
    "            # overwrite old files with new files containing the sentiment\n",
    "            with open(dir_list+file_name, \"w\") as f:\n",
    "                  json.dump(data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/store.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]\n",
    "    - azure-ai-m\n",
    "    - transformers\n",
    "    - sentencepiece\n",
    "    - pandas\n",
    "    - numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name stock-analysis-env is registered to workspace, the environment version is 1.0\n"
     ]
    }
   ],
   "source": [
    "custom_env_name = \"stock-analysis-env\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=\"1.0\")\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock analysis pipeline\",\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/python-sdk-v2:4\",\n",
    "        version=\"1.0\",\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = AssetTypes.URI_FILE\n",
    "mode = InputOutputModes.RO_MOUNT\n",
    "path = \"azureml://datastores/stocknewsjson/stock-news-json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads files from Azure Blob Storage from todays \",\n",
    "    inputs={\n",
    "        \"input_data\": Input(type=data_type, mode=mode),\n",
    "        \"folder_path\": Input(type=AssetTypes.URI_FOLDER, path=path)\n",
    "    },\n",
    "    outputs={\"output_data\": Output(type=data_type, mode=mode, path=path)},\n",
    "    code=\"./components/prep.py\",\n",
    "    command=\"python prep.py --input_data ${{inputs.input_data}} --output_data ${{outputs.output_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"input_data\": Input(type=data_type, mode=mode, path=path),\n",
    "        \"folder_path\": Input(type=AssetTypes.URI_FOLDER, path=path)\n",
    "    },\n",
    "    code=\"./components/classify.py\",\n",
    "    command=\"python get_data.py --input_data ${{inputs.input_data}} --train_data ${{outputs.train_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"input_data\": Input(type=data_type, mode=mode),\n",
    "        \"folder_path\": Input(type=AssetTypes.URI_FOLDER, path=path)\n",
    "    },\n",
    "    code=\"./components/summarize.py\",\n",
    "    command=\"python get_data.py --input_data ${{inputs.input_data}} --train_data ${{outputs.train_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline(compute=\"ava\")\n",
    "def pipeline_with_non_python_components(input_data):\n",
    "\n",
    "    data_prep_job = data_prep_component(input_data=input_data)\n",
    "    classify_job = classify_component(input_data=data_prep_job.outputs.train_data) # feed putput of previous step into the training job\n",
    "    summarize_job = summarize_component(input_data=data_prep_component.outputs.output_data)\n",
    "\n",
    "    return {\"out\": data_prep_job.outputs.train_data}\n",
    "\n",
    "\n",
    "pipeline_job = pipeline_with_non_python_components(\n",
    "    input_data=Input(\n",
    "        path=\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=IBMdatatype=csv&outputsize=full&apikey=SGXL42YQBJ7R7WXL\"\n",
    "        ) # stock data via AlphaVantage\n",
    "    )\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"ava\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
