{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import constants\n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=constants.SUBSCRIPTION_ID,\n",
    "    resource_group_name=constants.RESOURCE_GROUP_NAME,\n",
    "    workspace_name=constants.WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/prep.py \n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "# Check if given credential can get token successfully.\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "secret_client = SecretClient(vault_url=\"https://mlgroup.vault.azure.net/\", credential=credential)\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--blob_storage\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--prep_output\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# log in to the Blob Service Client\n",
    "blob_storage = args.blob_storage\n",
    "blob_storage_key = secret_client.get_secret(\"blob-storage-key\")\n",
    "blob_service_client = BlobServiceClient(blob_storage, account_key=blob_storage_key.value)\n",
    "\n",
    "# connect to the container \n",
    "container_client = blob_service_client.get_container_client(container=\"stock-news-json\") \n",
    "\n",
    "# list and download all currently available blobs\n",
    "blob_list = container_client.list_blobs()\n",
    "print(f\"Blob from: {blob_storage} has these blobs today: {blob_list}\")\n",
    "\n",
    "# get the timestamp with the current day \n",
    "current_day_timestamp = datetime.datetime.today().timestamp()\n",
    "current_day_timestamp = str(current_day_timestamp)[:5] # first 8 digits are the timestamp of the day\n",
    "\n",
    "blobs_to_use = [blob.name for blob in blob_list if current_day_timestamp in blob.name]\n",
    "for blob in blobs_to_use:\n",
    "      print(f\"Downloading blob: {blob}\")\n",
    "      blob_client = blob_service_client.get_blob_client(container=\"stock-news-json\", blob=blob)\n",
    "      with open(blob, mode=\"wb\") as sample_blob:\n",
    "            download_stream = blob_client.download_blob()\n",
    "            sample_blob.write(download_stream.readall())\n",
    "\n",
    "all_data_dict = {}\n",
    "for json_file in blobs_to_use:\n",
    "      with open(json_file,\"r+\") as file:\n",
    "      # First we load existing data into a dict.\n",
    "            file_data = json.load(file)\n",
    "            all_data_dict.update(file_data)\n",
    "            \n",
    "with open((Path(args.prep_output) / \"merged_stock_news.json\"), \"w\") as file:\n",
    "      file.write(json.dumps(all_data_dict, indent=4))\n",
    "\n",
    "# this is a comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/classify.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--classify_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--classify_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# download distilbert model from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "\n",
    "# retriev the list of blobs from the current day - input is a .txt file\n",
    "with open(os.path.join(args.classify_input, \"merged_stock_news.json\"), \"r\") as f:\n",
    "      data = json.load(f)\n",
    "texts = data[\"texts\"]\n",
    "\n",
    "\n",
    "sentiments = []\n",
    "for text in texts: \n",
    "      tokenized_text = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            is_split_into_words=False,\n",
    "            return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      outputs = model(tokenized_text[\"input_ids\"])\n",
    "      outputs_logits = outputs.logits.argmax(1)\n",
    "\n",
    "      mapping = {0: 'neutral', 1: 'negative', 2: 'positive'}\n",
    "      predicted_label = mapping[int(outputs_logits[0])]\n",
    "      sentiments.append(predicted_label)\n",
    "\n",
    "# add the sentiments to the data\n",
    "data[\"sentiments\"] = sentiments\n",
    "\n",
    "# overwrite old files with new files containing the sentiment\n",
    "with open((Path(args.classify_output) / \"merged_stock_news.json\"), \"w\") as f:\n",
    "      json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/summarize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/summarize.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--summarize_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--summarize_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the model and the tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\") \n",
    "\n",
    "# retriev the list of blobs from the current day - input is a .txt file\n",
    "with open(os.path.join(args.summarize_input, \"merged_stock_news.json\"), \"r\") as f:\n",
    "      data = json.load(f)\n",
    "texts = data[\"texts\"]\n",
    "\n",
    "summaries = []\n",
    "for text in texts: \n",
    "      # Tokenize our text\n",
    "      # If you want to run the code in Tensorflow, please remember to return the particular tensors as simply as using return_tensors = 'tf'\n",
    "      input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "      # Generate the output (Here, we use beam search but you can also use any other strategy you like)\n",
    "      output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=32, \n",
    "            num_beams=5, \n",
    "            early_stopping=True\n",
    "      )\n",
    "\n",
    "      # Finally, we can print the generated summary\n",
    "      summaries.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# add the sentiments to the data\n",
    "data[\"summaries\"] = summaries\n",
    "\n",
    "# overwrite old files with new files containing the sentiment\n",
    "with open((Path(args.summarize_output) / \"merged_stock_news.json\"), \"w\") as f:\n",
    "      json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/notify.py\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "# Check if given credential can get token successfully.\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "secret_client = SecretClient(vault_url=\"https://mlgroup.vault.azure.net/\", credential=credential)\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--notify_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--notify_output\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# get the timestamp with the current day \n",
    "current_day_timestamp = datetime.datetime.today().timestamp()\n",
    "\n",
    "with open(os.path.join(args.summarize_input, \"merged_stock_news.json\"), \"r\") as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "# analyze the results of the sentiment and\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = json.dumps(article_info)\n",
    "\n",
    "# connect and authenticate to the blob client\n",
    "account_url = \"https://mlstorageleo.blob.core.windows.net\"\n",
    "file_name = f\"processed-stock-news-{current_day_timestamp}.json\"\n",
    "\n",
    "# Create the BlobServiceClient object\n",
    "blob_storage_key = secret_client.get_secret(\"blob-storage-key\")\n",
    "blob_service_client = BlobServiceClient(account_url, credential=blob_storage_key.value)\n",
    "blob_client = blob_service_client.get_blob_client(container=\"processed-stock-news-json\", blob=file_name)\n",
    "blob_client.upload_blob(data)\n",
    "\n",
    "with open((Path(args.notify_output) / \"merged_stock_news.json\"), \"w\") as file:\n",
    "      file.write(json.dumps(all_data_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"data/processed-example.json\", \"r\") as f:\n",
    "      data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = data[\"sentiments\"]\n",
    "pos = sentiments.count(\"positive\")\n",
    "neu = sentiments.count(\"neutral\")\n",
    "neg = sentiments.count(\"negative\")\n",
    "ticker = data[\"ticker\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT: 6 positive, 1 neutral, 0 negative\n"
     ]
    }
   ],
   "source": [
    "sentiment_message = f\"{ticker}: {pos} positive, {neu} neutral, {neg} negative\"\n",
    "print(sentiment_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares of Texas Instruments Incorporated (TXN) have been showing 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299\n",
      "Analysts have provided the following ratings for Texas Instruments (:TXN).\n",
      "BBC Sport takes a look back at some of the more memorable moments from this year’s FA Cup.\n",
      "Over the past 20 years, Texas Instruments has returned 11.85% on average.\n",
      "Check out our latest analysis for Texas Instruments Return on Equity (ROE) weighs Texas Instruments' profit against the level of shareholders' equity.\n",
      "Texas Instruments Incorporated (TXN) shares up by 0.48% in previous day’s close.\n",
      "All photographs subject to copyright.\n"
     ]
    }
   ],
   "source": [
    "summaries = \"\\n\".join(data[\"summaries\"])\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: stock-analysis-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - azure-storage-blob\n",
    "    - azure-identity\n",
    "    - azure-keyvault\n",
    "    - transformers\n",
    "    - torch\n",
    "    - sentencepiece\n",
    "    - numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env_name = \"stock-analysis-env\"\n",
    "version = \"1.6\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=version)\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock analysis pipeline\",\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/python-sdk-v2:4\",\n",
    "        version=version,\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = AssetTypes.URI_FOLDER\n",
    "path = \"azureml://datastores/stocknewsjson/stock-news-json\"\n",
    "input_mode = InputOutputModes.RO_MOUNT\n",
    "output_mode = InputOutputModes.RW_MOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Finding out which blobs to actually use\",\n",
    "    description=\"Loads files from Azure Blob Storage from todays \",\n",
    "    inputs={\n",
    "        \"blob_storage\": Input(mode=InputOutputModes.DIRECT)\n",
    "    },\n",
    "    outputs={\n",
    "        \"prep_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/prep.py\",\n",
    "    command=\"python prep.py --blob_storage ${{inputs.blob_storage}} --prep_output ${{outputs.prep_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Classify the sentiments of todays stock news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"classify_input\": Input(type=data_type, mode=input_mode), \n",
    "    },\n",
    "    outputs={\n",
    "        \"classify_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/classify.py\",\n",
    "    command=\"python classify.py --classify_input ${{inputs.classify_input}} --classify_output ${{outputs.classify_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Summarize the news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"summarize_input\": Input(type=data_type, mode=input_mode),\n",
    "    },\n",
    "    outputs={\n",
    "        \"summarize_output\": Output(type=data_type, mode=output_mode)\n",
    "    },\n",
    "    code=\"./components/summarize.py\",\n",
    "    command=\"python summarize.py --summarize_input ${{inputs.summarize_input}} --summarize_output ${{outputs.summarize_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    "    is_deterministic=\"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline(compute=\"ava\")\n",
    "def stock_news_pipeline():\n",
    "\n",
    "    data_prep_job = data_prep_component(\n",
    "        blob_storage=\"https://mlstorageleo.blob.core.windows.net/\"\n",
    "    )\n",
    "    classify_job = classify_component(\n",
    "        classify_input=data_prep_job.outputs.prep_output\n",
    "\n",
    "    ) # feed putput of previous step into the training job\n",
    "    summarize_job = summarize_component(\n",
    "        summarize_input = classify_job.outputs.classify_output\n",
    "    )\n",
    "\n",
    "    return {\"processed_file\": summarize_job.outputs.summarize_output}\n",
    "\n",
    "pipeline_job = stock_news_pipeline()\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"ava\"\n",
    "pipeline_job.settings.reuse_component = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading classify.py\u001b[32m (< 1 MB): 100%|##########| 1.53k/1.53k [00:00<00:00, 21.1kB/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading summarize.py\u001b[32m (< 1 MB): 100%|##########| 1.82k/1.82k [00:00<00:00, 28.6kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>stock-news-analysis-pipeline</td><td>honest_tangelo_w4jjb1dppp</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/honest_tangelo_w4jjb1dppp?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace&amp;tid=08548f02-0216-4325-938b-fd30f6829e55\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {'processed_file': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x000002E08E02D9D0>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002E08E02D7C0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'stock_news_pipeline', 'is_deterministic': None, 'inputs': {}, 'outputs': {'processed_file': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'data_prep_job': Command({'parameters': {}, 'init': False, 'name': 'data_prep_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002E08E04E250>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Finding out which blobs to actually use', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'blob_storage': {'type': 'uri_folder', 'path': 'https://mlstorageleo.blob.core.windows.net/'}}, 'job_outputs': {'prep_output': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'blob_storage': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002E08E04EE80>}, 'outputs': {'prep_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002E08E04E3A0>}, 'component': 'azureml_anonymous:f09f1407-0afd-4825-a96f-583c91aa51c4', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '032c14af-0d99-4d43-9592-9272ae9c5adc', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'classify_job': Command({'parameters': {}, 'init': False, 'name': 'classify_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002E08E04EDC0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Classify the sentiments of todays stock news', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'classify_input': '${{parent.jobs.data_prep_job.outputs.prep_output}}'}, 'job_outputs': {'classify_output': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'classify_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002E08E04E430>}, 'outputs': {'classify_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002E08F645D30>}, 'component': 'azureml_anonymous:c6ef373a-9238-471e-aeff-f81556ec46d2', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'b90ccf79-586a-4d11-a611-1d0b5fdf9b09', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'summarize_job': Command({'parameters': {}, 'init': False, 'name': 'summarize_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002E08E02D1F0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Summarize the news', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'summarize_input': '${{parent.jobs.classify_job.outputs.classify_output}}'}, 'job_outputs': {'summarize_output': '${{parent.outputs.processed_file}}'}, 'inputs': {'summarize_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002E08E02D910>}, 'outputs': {'summarize_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002E08E02D8E0>}, 'component': 'azureml_anonymous:f6d60c9d-cbcf-4ce8-a878-6e3ea8480d1c', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'eefcb077-61f5-4f8f-9672-7bfd55759f67', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 3}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 3}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'honest_tangelo_w4jjb1dppp', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'git@github.com:LeonardPuettmann/azure-stock-news-analysis.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '931cfec663f1807fe9328e4a091ff1203d5fed35', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'ava', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/mlgroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace/jobs/honest_tangelo_w4jjb1dppp', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000002E08E02DBB0>, 'serialize': <msrest.serialization.Serializer object at 0x000002E08E02D250>, 'display_name': 'stock_news_pipeline', 'experiment_name': 'stock-news-analysis-pipeline', 'compute': 'ava', 'services': {'Tracking': {'endpoint': 'azureml://northeurope.api.azureml.ms/mlflow/v1.0/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/mlgroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/honest_tangelo_w4jjb1dppp?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace&tid=08548f02-0216-4325-938b-fd30f6829e55', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"stock-news-analysis-pipeline\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: honest_tangelo_w4jjb1dppp\n",
      "Web View: https://ml.azure.com/runs/honest_tangelo_w4jjb1dppp?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-07-09 19:37:12Z] Submitting 1 runs, first five are: 73d93c12:de61c408-4d35-42e4-ac58-dc7ea6500732\n",
      "[2023-07-09 19:37:39Z] Completing processing run id de61c408-4d35-42e4-ac58-dc7ea6500732.\n",
      "[2023-07-09 19:37:39Z] Submitting 1 runs, first five are: 27a9ddf8:3f0cb8bb-a1a6-4246-823e-6af3ff7ec8b9\n",
      "[2023-07-09 19:38:29Z] Completing processing run id 3f0cb8bb-a1a6-4246-823e-6af3ff7ec8b9.\n",
      "[2023-07-09 19:38:30Z] Submitting 1 runs, first five are: 9c985e6c:16a065ed-01f4-4b51-802b-34a16f7444d0\n",
      "[2023-07-09 19:43:50Z] Completing processing run id 16a065ed-01f4-4b51-802b-34a16f7444d0.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: honest_tangelo_w4jjb1dppp\n",
      "Web View: https://ml.azure.com/runs/honest_tangelo_w4jjb1dppp?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
