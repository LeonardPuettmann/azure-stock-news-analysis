{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import constants\n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=constants.SUBSCRIPTION_ID,\n",
    "    resource_group_name=constants.RESOURCE_GROUP_NAME,\n",
    "    workspace_name=constants.WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/prep.py \n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--blob_storage_read\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--account_url\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# log in to the Blob Service Client\n",
    "account_url = args.account_url\n",
    "blob_service_client = BlobServiceClient(account_url, account_key=constants.BLOB_KEY)\n",
    "\n",
    "# connect to the container \n",
    "container_client = blob_service_client.get_container_client(container=\"stock-news-json\") \n",
    "\n",
    "# list and download all currently available blobs\n",
    "blob_list = container_client.list_blobs()\n",
    "\n",
    "# get the timestamp with the current day \n",
    "current_day_timestamp = datetime.datetime.today().timestamp()\n",
    "current_day_timestamp = str(current_day_timestamp)[:8] # first 8 digits are the timestamp of the day\n",
    "\n",
    "blobs_to_use = [blob.name for blob in blob_list if current_day_timestamp in blob.name]\n",
    "\n",
    "# # ! the files should not be downloaded in this step. Instead it might make more sense to pass a list with the filenames to the next component\n",
    "# with open(args.blob_storage+\"/blobs_to_use.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(blob for blob in blobs_to_use), f)\n",
    "\n",
    "(Path(args.score_output) / \"blobs_to_use.txt\").write_text(\"\\n\".join(blob for blob in blobs_to_use))\n",
    "# continue here with this example -> https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/1a_pipeline_with_components_from_yaml/score_src/score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/classify.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--blob_storage_read\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--blobs_to_use_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--blob_storage_write\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# download distilbert model from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "\n",
    "# retriev the list of blobs from the current day - input is a .txt file\n",
    "with open(args.blob_storage, \"r\") as f:\n",
    "      blobs_to_use = f.read()\n",
    "\n",
    "\n",
    "dir_list = args.folder_path\n",
    "for file_name in [file for file in os.listdir(dir_list) if file in blobs_to_use]:\n",
    "      with open(dir_list + file_name) as json_file:\n",
    "            data = json.load(json_file)\n",
    "      texts = data[\"texts\"]\n",
    "\n",
    "      sentiments = []\n",
    "      for text in texts: \n",
    "            tokenized_text = tokenizer(\n",
    "                  text,\n",
    "                  truncation=True,\n",
    "                  is_split_into_words=False,\n",
    "                  return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            outputs = model(tokenized_text[\"input_ids\"])\n",
    "            outputs_logits = outputs.logits.argmax(1)\n",
    "\n",
    "            mapping = {0: 'neutral', 1: 'negative', 2: 'positive'}\n",
    "            predicted_label = mapping[int(outputs_logits[0])]\n",
    "            sentiments.append(predicted_label)\n",
    "\n",
    "      # add the sentiments to the data\n",
    "      data[\"sentiments\"] = sentiments\n",
    "\n",
    "      # overwrite old files with new files containing the sentiment\n",
    "      with open(dir_list+file_name, \"w\") as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/summarize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/summarize.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the model and the tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\") \n",
    "\n",
    "dir_list = args.blob_storage\n",
    "for file_name in [file for file in os.listdir(dir_list) if file.endswith('.json')]:\n",
    "      with open(dir_list + file_name) as json_file:\n",
    "            data = json.load(json_file)\n",
    "      texts = data[\"texts\"]\n",
    "\n",
    "      summaries = []\n",
    "      for text in texts: \n",
    "            # Tokenize our text\n",
    "            # If you want to run the code in Tensorflow, please remember to return the particular tensors as simply as using return_tensors = 'tf'\n",
    "            input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            # Generate the output (Here, we use beam search but you can also use any other strategy you like)\n",
    "            output = model.generate(\n",
    "                  input_ids, \n",
    "                  max_length=32, \n",
    "                  num_beams=5, \n",
    "                  early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Finally, we can print the generated summary\n",
    "            summaries.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "      # add the sentiments to the data\n",
    "      data[\"summaries\"] = summaries\n",
    "\n",
    "      # overwrite old files with new files containing the sentiment\n",
    "      with open(dir_list+file_name, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "(Path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: stock-analysis-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - azure-storage\n",
    "    - transformers\n",
    "    - sentencepiece\n",
    "    - numpy\n",
    "    - json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env_name = \"stock-analysis-env\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=\"1.1\")\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock analysis pipeline\",\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/python-sdk-v2:4\",\n",
    "        version=\"1.1\",\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = AssetTypes.URI_FOLDER\n",
    "mode = InputOutputModes.RW_MOUNT\n",
    "path = \"azureml://datastores/stocknewsjson/stock-news-json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Finding out which blobs to actually use\",\n",
    "    description=\"Loads files from Azure Blob Storage from todays \",\n",
    "    inputs={\n",
    "        \"blob_storage_read\": Input(type=data_type, mode=InputOutputModes.RO_MOUNT, path=path),\n",
    "        \"account_url\": Input(mode=InputOutputModes.DIRECT)\n",
    "    },\n",
    "    outputs={\n",
    "        \"blobs_to_use_output\": Output(type=data_type, mode=InputOutputModes.RW_MOUNT, path=path)\n",
    "    },\n",
    "    code=\"./components/prep.py\",\n",
    "    command=\"python prep.py --blob_storage ${{inputs.blob_storage}} --blobs_to_use_output ${{outputs.blobs_to_use_output}}, --account_url ${{inputs.account_url}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Classify the sentiments of todays stock news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"blob_storage_read\": Input(type=data_type, mode=InputOutputModes.RO_MOUNT, path=path), \n",
    "        \"blobs_to_use_input\": Input(type=AssetTypes.URI_FILE),\n",
    "    },\n",
    "    outputs={\n",
    "        \"blob_storage_write\": Output(type=data_type, mode=InputOutputModes.RW_MOUNT, path=path)\n",
    "    },\n",
    "    code=\"./components/classify.py\",\n",
    "    command=\"python classify.py --blob_storage_read ${{inputs.blob_storage_read}} --blobs_to_use_input ${{inputs.blobs_to_use_input}} --blob_storage_write ${{outputs.blob_storage_write}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Summarize the news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"blob_storage_read\": Input(type=data_type, mode=InputOutputModes.RO_MOUNT, path=path),\n",
    "        \"blobs_to_use_input\": Input(type=AssetTypes.URI_FILE)\n",
    "    },\n",
    "    outputs={\n",
    "        \"blob_storage_write\": Output(type=data_type, mode=InputOutputModes.RW_MOUNT, path=path)\n",
    "    },\n",
    "    code=\"./components/summarize.py\",\n",
    "    command=\"python summarize.py --blob_storage_read ${{inputs.blob_storage_read}} --blob_storage_write ${{outputs.blob_storage_write}} --blobs_to_use_input ${{inputs.blobs_to_use_input}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedKeywordError",
     "evalue": "[component] Classify the sentiments of todays stock news() got an unexpected keyword argument 'blob_storage_write', valid keywords: 'blob_storage_read', 'blobs_to_use_input'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedKeywordError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m     summarize_job \u001b[39m=\u001b[39m summarize_component(\n\u001b[0;32m     15\u001b[0m         blob_storage_read\u001b[39m=\u001b[39mpath, \n\u001b[0;32m     16\u001b[0m         blob_storage_write\u001b[39m=\u001b[39mpath\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mblobs_processed\u001b[39m\u001b[39m\"\u001b[39m: data_prep_job\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mblobs_to_use_output}\n\u001b[1;32m---> 21\u001b[0m pipeline_job \u001b[39m=\u001b[39m pipeline_with_python_function_components(\n\u001b[0;32m     22\u001b[0m     account_url\u001b[39m=\u001b[39;49mInput(\n\u001b[0;32m     23\u001b[0m         path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://mlstorageleo.blob.core.windows.net\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[39m# set pipeline level compute\u001b[39;00m\n\u001b[0;32m     28\u001b[0m pipeline_job\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mdefault_compute \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mava\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\dsl\\_pipeline_decorator.py:180\u001b[0m, in \u001b[0;36mpipeline.<locals>.pipeline_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     non_pipeline_params_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m    176\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m provided_positional_kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m non_pipeline_inputs\n\u001b[0;32m    177\u001b[0m     }\n\u001b[0;32m    179\u001b[0m     \u001b[39m# TODO: cache built pipeline component\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     pipeline_component \u001b[39m=\u001b[39m pipeline_builder\u001b[39m.\u001b[39;49mbuild(\n\u001b[0;32m    181\u001b[0m         user_provided_kwargs\u001b[39m=\u001b[39;49mprovided_positional_kwargs,\n\u001b[0;32m    182\u001b[0m         non_pipeline_inputs_dict\u001b[39m=\u001b[39;49mnon_pipeline_params_dict,\n\u001b[0;32m    183\u001b[0m         non_pipeline_inputs\u001b[39m=\u001b[39;49mnon_pipeline_inputs,\n\u001b[0;32m    184\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[39m# use `finally` to ensure pop operation from the stack\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     dsl_settings \u001b[39m=\u001b[39m _dsl_settings_stack\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\dsl\\_pipeline_component_builder.py:180\u001b[0m, in \u001b[0;36mPipelineComponentBuilder.build\u001b[1;34m(self, user_provided_kwargs, non_pipeline_inputs_dict, non_pipeline_inputs)\u001b[0m\n\u001b[0;32m    177\u001b[0m _definition_builder_stack\u001b[39m.\u001b[39mpush(\u001b[39mself\u001b[39m)\n\u001b[0;32m    179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     outputs, _locals \u001b[39m=\u001b[39m get_outputs_and_locals(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, kwargs)\n\u001b[0;32m    181\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     _definition_builder_stack\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\_utils\\_func_utils.py:407\u001b[0m, in \u001b[0;36mget_outputs_and_locals\u001b[1;34m(func, _all_kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_outputs_and_locals\u001b[39m(func, _all_kwargs):\n\u001b[0;32m    398\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get outputs and locals from self.func. Locals will be used to update node variable names.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[39m    :param func: The function to execute.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m    :rtype: typing.Tuple[typing.Dict, typing.Dict]\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_persistent_locals_builder()\u001b[39m.\u001b[39;49mcall(func, _all_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\_utils\\_func_utils.py:57\u001b[0m, in \u001b[0;36mPersistentLocalsFunctionBuilder.call\u001b[1;34m(self, func, _all_kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minjected_param \u001b[39min\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m.\u001b[39mco_varnames:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_error(\u001b[39m\"\u001b[39m\u001b[39mconflict_argument\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m.\u001b[39mco_varnames)))\n\u001b[1;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(func, _all_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\_utils\\_func_utils.py:93\u001b[0m, in \u001b[0;36mPersistentLocalsFunctionProfilerBuilder._call\u001b[1;34m(self, func, _all_kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m func_variable_profiler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_func_variable_tracer(_locals, func\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_sys_profiler(func_variable_profiler):\n\u001b[1;32m---> 93\u001b[0m     outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_all_kwargs)\n\u001b[0;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m outputs, _locals\n",
      "Cell \u001b[1;32mIn[68], line 9\u001b[0m, in \u001b[0;36mpipeline_with_python_function_components\u001b[1;34m(account_url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m@pipeline\u001b[39m(compute\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mava\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpipeline_with_python_function_components\u001b[39m(account_url):\n\u001b[0;32m      6\u001b[0m     data_prep_job \u001b[39m=\u001b[39m data_prep_component(\n\u001b[0;32m      7\u001b[0m         account_url\u001b[39m=\u001b[39maccount_url\n\u001b[0;32m      8\u001b[0m     )\n\u001b[1;32m----> 9\u001b[0m     classify_job \u001b[39m=\u001b[39m classify_component(\n\u001b[0;32m     10\u001b[0m         blobs_to_use_input\u001b[39m=\u001b[39;49mdata_prep_job\u001b[39m.\u001b[39;49moutputs\u001b[39m.\u001b[39;49mblobs_to_use_output,\n\u001b[0;32m     11\u001b[0m         blob_storage_read\u001b[39m=\u001b[39;49mpath, \n\u001b[0;32m     12\u001b[0m         blob_storage_write\u001b[39m=\u001b[39;49mpath\n\u001b[0;32m     13\u001b[0m     ) \u001b[39m# feed putput of previous step into the training job\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     summarize_job \u001b[39m=\u001b[39m summarize_component(\n\u001b[0;32m     15\u001b[0m         blob_storage_read\u001b[39m=\u001b[39mpath, \n\u001b[0;32m     16\u001b[0m         blob_storage_write\u001b[39m=\u001b[39mpath\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mblobs_processed\u001b[39m\u001b[39m\"\u001b[39m: data_prep_job\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mblobs_to_use_output}\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\entities\\_builders\\command.py:710\u001b[0m, in \u001b[0;36mCommand.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call Command as a function will return a new instance each time.\"\"\"\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_component, Component):\n\u001b[0;32m    709\u001b[0m     \u001b[39m# call this to validate inputs\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_component(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    711\u001b[0m     \u001b[39m# merge inputs\u001b[39;00m\n\u001b[0;32m    712\u001b[0m     \u001b[39mfor\u001b[39;00m name, original_input \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\entities\\_component\\component.py:558\u001b[0m, in \u001b[0;36mComponent.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    549\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mComponent function doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt has any parameters, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplease make sure component \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m has inputs. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m         )\n\u001b[0;32m    552\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationException(\n\u001b[0;32m    553\u001b[0m         message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    554\u001b[0m         target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mCOMPONENT,\n\u001b[0;32m    555\u001b[0m         no_personal_data_message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    556\u001b[0m         error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mUSER_ERROR,\n\u001b[0;32m    557\u001b[0m     )\n\u001b[1;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\dsl\\_dynamic.py:125\u001b[0m, in \u001b[0;36m[component] Classify the sentiments of todays stock news\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m     \u001b[39m# We need to make sure all keys of kwargs are valid.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[39m# Merge valid group keys with original keys.\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     _assert_arg_valid(kwargs, [\u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(default_kwargs\u001b[39m.\u001b[39mkeys()), \u001b[39m*\u001b[39mflattened_group_keys], func_name\u001b[39m=\u001b[39mfunc_name)\n\u001b[0;32m    126\u001b[0m     \u001b[39m# We need to put the default args to the kwargs before invoking the original function.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     _update_dct_if_not_exist(kwargs, default_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\dsl\\_dynamic.py:91\u001b[0m, in \u001b[0;36m_assert_arg_valid\u001b[1;34m(kwargs, keys, func_name)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 module_logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     88\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mComponent input name \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, treat it as \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, key, lower2original_parameter_names[lower_key]\n\u001b[0;32m     89\u001b[0m                 )\n\u001b[0;32m     90\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m             \u001b[39mraise\u001b[39;00m UnexpectedKeywordError(func_name\u001b[39m=\u001b[39mfunc_name, keyword\u001b[39m=\u001b[39mkey, keywords\u001b[39m=\u001b[39mkeys)\n\u001b[0;32m     92\u001b[0m \u001b[39m# update kwargs to align with yaml definition\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs_need_to_update:\n",
      "\u001b[1;31mUnexpectedKeywordError\u001b[0m: [component] Classify the sentiments of todays stock news() got an unexpected keyword argument 'blob_storage_write', valid keywords: 'blob_storage_read', 'blobs_to_use_input'."
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline(compute=\"ava\")\n",
    "def pipeline_with_python_function_components(account_url):\n",
    "\n",
    "    data_prep_job = data_prep_component(\n",
    "        account_url=account_url\n",
    "    )\n",
    "    classify_job = classify_component(\n",
    "        blobs_to_use_input=data_prep_job.outputs.blobs_to_use_output,\n",
    "        blob_storage_read=path, \n",
    "        blob_storage_write=path\n",
    "    ) # feed putput of previous step into the training job\n",
    "    summarize_job = summarize_component(\n",
    "        blob_storage_read=path, \n",
    "        blob_storage_write=path\n",
    "    )\n",
    "\n",
    "    return {\"blobs_processed\": data_prep_job.outputs.blobs_to_use_output}\n",
    "\n",
    "pipeline_job = pipeline_with_python_function_components(\n",
    "    account_url=Input(\n",
    "        path=\"https://mlstorageleo.blob.core.windows.net\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"ava\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "\n\u001b[37m\n\u001b[30m\n1) At least one required parameter is missing\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Input path can't be empty for jobs.\u001b[39m\n\nResolutions: \n1) Ensure all parameters required by the Job schema are specified.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://docs.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:541\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[1;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39m# Create all dependent resources\u001b[39;00m\n\u001b[1;32m--> 541\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_arm_id_or_upload_dependencies(job)\n\u001b[0;32m    543\u001b[0m git_props \u001b[39m=\u001b[39m get_git_properties()\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:890\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_upload_dependencies\u001b[1;34m(self, job)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This method converts name or name:version to ARM id. Or it\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[39mregisters/uploads nested dependencies.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[39m:rtype: Job\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 890\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_arm_id_or_azureml_id(job, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orchestrators\u001b[39m.\u001b[39;49mget_asset_arm_id)\n\u001b[0;32m    892\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(job, PipelineJob):\n\u001b[0;32m    893\u001b[0m     \u001b[39m# Resolve top-level inputs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1115\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_azureml_id\u001b[1;34m(self, job, resolver)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(job, PipelineJob):\n\u001b[1;32m-> 1115\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_arm_id_for_pipeline_job(job, resolver)\n\u001b[0;32m   1116\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1230\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_for_pipeline_job\u001b[1;34m(self, pipeline_job, resolver)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_component_operations\u001b[39m.\u001b[39;49m_resolve_dependencies_for_pipeline_component_jobs(\n\u001b[0;32m   1231\u001b[0m         pipeline_job\u001b[39m.\u001b[39;49mcomponent, resolver\n\u001b[0;32m   1232\u001b[0m     )\n\u001b[0;32m   1233\u001b[0m \u001b[39mexcept\u001b[39;00m ComponentException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_component_operations.py:741\u001b[0m, in \u001b[0;36mComponentOperations._resolve_dependencies_for_pipeline_component_jobs\u001b[1;34m(self, component, resolver, resolve_inputs)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[39mif\u001b[39;00m resolve_inputs:\n\u001b[1;32m--> 741\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_inputs_for_pipeline_component_jobs(component\u001b[39m.\u001b[39;49mjobs, component\u001b[39m.\u001b[39;49m_base_path)\n\u001b[0;32m    743\u001b[0m \u001b[39m# This is a preparation for concurrent resolution. Nodes will be resolved later layer by layer\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[39m# from bottom to top, as hash calculation of a parent node will be impacted by resolution\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[39m# of its child nodes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_component_operations.py:568\u001b[0m, in \u001b[0;36mComponentOperations._resolve_inputs_for_pipeline_component_jobs\u001b[1;34m(self, jobs, base_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m     node: BaseNode \u001b[39m=\u001b[39m job_instance\n\u001b[1;32m--> 568\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_job_operations\u001b[39m.\u001b[39;49m_resolve_job_inputs(\n\u001b[0;32m    569\u001b[0m         \u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49m_data, node\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mvalues()),\n\u001b[0;32m    570\u001b[0m         base_path,\n\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    572\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(job_instance, AutoMLJob):\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:972\u001b[0m, in \u001b[0;36mJobOperations._resolve_job_inputs\u001b[1;34m(self, entries, base_path)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m entries:\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_job_input(entry, base_path)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:995\u001b[0m, in \u001b[0;36mJobOperations._resolve_job_input\u001b[1;34m(self, entry, base_path)\u001b[0m\n\u001b[0;32m    994\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInput path can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be empty for jobs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 995\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationException(\n\u001b[0;32m    996\u001b[0m         message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    997\u001b[0m         target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mJOB,\n\u001b[0;32m    998\u001b[0m         no_personal_data_message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    999\u001b[0m         error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mUSER_ERROR,\n\u001b[0;32m   1000\u001b[0m         error_type\u001b[39m=\u001b[39mValidationErrorType\u001b[39m.\u001b[39mMISSING_FIELD,\n\u001b[0;32m   1001\u001b[0m     )\n\u001b[0;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1004\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(entry, Input)\n\u001b[0;32m   1005\u001b[0m     \u001b[39mor\u001b[39;00m is_ARM_id_for_resource(entry\u001b[39m.\u001b[39mpath)\n\u001b[0;32m   1006\u001b[0m     \u001b[39mor\u001b[39;00m is_url(entry\u001b[39m.\u001b[39mpath)\n\u001b[0;32m   1007\u001b[0m     \u001b[39mor\u001b[39;00m is_data_binding_expression(entry\u001b[39m.\u001b[39mpath)  \u001b[39m# literal value but set mode in pipeline yaml\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m ):  \u001b[39m# Literal value, ARM id or remote url. Pass through\u001b[39;00m\n",
      "\u001b[1;31mValidationException\u001b[0m: Input path can't be empty for jobs.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# submit job to workspace\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pipeline_job \u001b[39m=\u001b[39m ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mcreate_or_update(\n\u001b[0;32m      3\u001b[0m     pipeline_job, experiment_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstock-news-analysis-pipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m pipeline_job\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:337\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m dimensions \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparameter_dimensions, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(custom_dimensions \u001b[39mor\u001b[39;00m {})}\n\u001b[0;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, dimensions) \u001b[39mas\u001b[39;00m activityLogger:\n\u001b[1;32m--> 337\u001b[0m     return_value \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parameter_dimensions:\n\u001b[0;32m    339\u001b[0m         \u001b[39m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         activityLogger\u001b[39m.\u001b[39mactivity_info\u001b[39m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:607\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[1;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarshmallow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m ValidationError \u001b[39mas\u001b[39;00m SchemaValidationError\n\u001b[0;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ex, (ValidationException, SchemaValidationError)):\n\u001b[1;32m--> 607\u001b[0m     log_and_raise_error(ex)\n\u001b[0;32m    608\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    609\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Users\\Leo\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\_exception_helper.py:295\u001b[0m, in \u001b[0;36mlog_and_raise_error\u001b[1;34m(error, debug, yaml_operation)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    293\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m--> 295\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(formatted_error)\n",
      "\u001b[1;31mException\u001b[0m: \n\u001b[37m\n\u001b[30m\n1) At least one required parameter is missing\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Input path can't be empty for jobs.\u001b[39m\n\nResolutions: \n1) Ensure all parameters required by the Job schema are specified.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://docs.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"stock-news-analysis-pipeline\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Wait until the job completes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ml_client\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mstream(pipeline_job\u001b[39m.\u001b[39;49mname)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
