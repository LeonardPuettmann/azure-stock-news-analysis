{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import constants\n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=constants.SUBSCRIPTION_ID,\n",
    "    resource_group_name=constants.RESOURCE_GROUP_NAME,\n",
    "    workspace_name=constants.WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/prep.py \n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(\"prep\")\n",
    "    parser.add_argument(\"--input_data\", type=str, help=\"Path of prepped data\")\n",
    "    parser.add_argument(\"--ougput_data\", type=str, help=\"Path of prepped data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # log in to the Blob Service Client\n",
    "    account_url = \"https://mlstorageleo.blob.core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient(account_url, account_key=constants.BLOB_KEY)\n",
    "\n",
    "    # connect to the container \n",
    "    container_client = blob_service_client.get_container_client(container=\"stock-news-json\") \n",
    "\n",
    "    # list and download all currently available blobs\n",
    "    blob_list = container_client.list_blobs()\n",
    "\n",
    "    # get the timestamp with the current day \n",
    "    current_day_timestamp = datetime.datetime.today().timestamp()\n",
    "    current_day_timestamp = str(current_day_timestamp)[:8] # first 8 digits are the timestamp of the day\n",
    "\n",
    "    blobs_to_download = [blob.name for blob in blob_list if current_day_timestamp in blob.name]\n",
    "    for blob in blobs_to_download:\n",
    "        download_file_path = os.path.join(args.prep_data, str(blob))\n",
    "        with open(file=download_file_path, mode=\"wb\") as download_file:\n",
    "            download_file.write(container_client.download_blob(blob).readall())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/classify.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"path or URL to input data\")\n",
    "parser.add_argument(\"--output_data\", type=str, help=\"path or URL to output data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# download distilbert model from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "\n",
    "def main():\n",
    "      #dir_list = os.listdir(args.input_data)\n",
    "      dir_list = args.input_data\n",
    "      for file_name in [file for file in os.listdir(dir_list) if file.endswith('.json')]:\n",
    "            with open(dir_list + file_name) as json_file:\n",
    "                  data = json.load(json_file)\n",
    "            texts = data[\"texts\"]\n",
    "\n",
    "            sentiments = []\n",
    "            for text in texts: \n",
    "                  tokenized_text = tokenizer(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        is_split_into_words=False,\n",
    "                        return_tensors=\"pt\"\n",
    "                  )\n",
    "\n",
    "                  outputs = model(tokenized_text[\"input_ids\"])\n",
    "                  outputs_logits = outputs.logits.argmax(1)\n",
    "\n",
    "                  mapping = {0: 'neutral', 1: 'negative', 2: 'positive'}\n",
    "                  predicted_label = mapping[int(outputs_logits[0])]\n",
    "                  sentiments.append(predicted_label)\n",
    "\n",
    "            # add the sentiments to the data\n",
    "            data[\"sentiments\"] = sentiments\n",
    "\n",
    "            # overwrite old files with new files containing the sentiment\n",
    "            with open(dir_list+file_name, \"w\") as f:\n",
    "                  json.dump(data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/summarize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/summarize.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"path or URL to input data\")\n",
    "parser.add_argument(\"--output_data\", type=str, help=\"path or URL to output data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the model and the tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\") \n",
    "\n",
    "def main():\n",
    "      #dir_list = os.listdir(args.input_data)\n",
    "      dir_list = args.input_data\n",
    "      for file_name in [file for file in os.listdir(dir_list) if file.endswith('.json')]:\n",
    "            with open(dir_list + file_name) as json_file:\n",
    "                  data = json.load(json_file)\n",
    "            texts = data[\"texts\"]\n",
    "\n",
    "            summaries = []\n",
    "            for text in texts: \n",
    "                # Tokenize our text\n",
    "                # If you want to run the code in Tensorflow, please remember to return the particular tensors as simply as using return_tensors = 'tf'\n",
    "                input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "                # Generate the output (Here, we use beam search but you can also use any other strategy you like)\n",
    "                output = model.generate(\n",
    "                    input_ids, \n",
    "                    max_length=32, \n",
    "                    num_beams=5, \n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "                # Finally, we can print the generated summary\n",
    "                summaries.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "            # add the sentiments to the data\n",
    "            data[\"summaries\"] = summaries\n",
    "\n",
    "            # overwrite old files with new files containing the sentiment\n",
    "            with open(dir_list+file_name, \"w\") as f:\n",
    "                  json.dump(data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/store.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]\n",
    "    - azure-ai-m\n",
    "    - transformers\n",
    "    - sentencepiece\n",
    "    - pandas\n",
    "    - numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name stock-analysis-env is registered to workspace, the environment version is 1.0\n"
     ]
    }
   ],
   "source": [
    "custom_env_name = \"stock-analysis-env\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=\"1.0\")\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock analysis pipeline\",\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/python-sdk-v2:4\",\n",
    "        version=\"1.0\",\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = AssetTypes.URI_FILE\n",
    "mode = InputOutputModes.RO_MOUNT\n",
    "path = \"https://mlstorageleo.blob.core.windows.net/stock-news-json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads files from Azure Blob Storage from todays \",\n",
    "    inputs={\"input_data\": Input(type=data_type, mode=mode, path=path)},\n",
    "    outputs={\"output_data\": Output(type=data_type, mode=mode, path=path)},\n",
    "    code=\"./components/prep.py\",\n",
    "    command=\"python prep.py --input_data ${{inputs.input_data}} --output_data ${{outputs.output_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\"input_data\": Input(type=data_type, mode=mode, path=path)},\n",
    "    outputs={\"train_data\": Output(type=data_type, mode=mode, path=path)},\n",
    "    code=\"./components/classify.py\",\n",
    "    command=\"python get_data.py --input_data ${{inputs.input_data}} --train_data ${{outputs.train_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\"input_data\": Input(type=data_type, mode=mode, path=path)},\n",
    "    outputs={\"train_data\": Output(type=data_type, mode=mode, path=path)},\n",
    "    code=\"./components/summarize.py\",\n",
    "    command=\"python get_data.py --input_data ${{inputs.input_data}} --train_data ${{outputs.train_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
