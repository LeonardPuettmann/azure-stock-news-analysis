{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import constants\n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=constants.SUBSCRIPTION_ID,\n",
    "    resource_group_name=constants.RESOURCE_GROUP_NAME,\n",
    "    workspace_name=constants.WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/prep.py \n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import ManagedIdentityCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "credential = ManagedIdentityCredential(client_id=\"a4d3af98-d651-42db-850f-485cbe770757\")\n",
    "secret_client = SecretClient(vault_url=\"https://mlgroupvault.vault.azure.net/\", credential=credential)\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--blob_storage\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--prep_output\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# log in to the Blob Service Client\n",
    "blob_storage = args.blob_storage\n",
    "blob_storage_key = secret_client.get_secret(\"blob-storage-key\")\n",
    "blob_service_client = BlobServiceClient(blob_storage, account_key=blob_storage_key.value)\n",
    "\n",
    "# connect to the container \n",
    "container_client = blob_service_client.get_container_client(container=\"stock-news-json\") \n",
    "\n",
    "# list and download all currently available blobs\n",
    "blob_list = container_client.list_blobs()\n",
    "print(f\"Blob from: {blob_storage} has these blobs today: {blob_list}\")\n",
    "\n",
    "# get the timestamp with the current day \n",
    "current_day_timestamp = datetime.datetime.today().timestamp()\n",
    "current_day_timestamp = str(current_day_timestamp)[:5] # first 8 digits are the timestamp of the day\n",
    "\n",
    "blobs_to_use = [blob.name for blob in blob_list if current_day_timestamp in blob.name]\n",
    "for blob in blobs_to_use:\n",
    "      print(f\"Downloading blob: {blob}\")\n",
    "      blob_client = blob_service_client.get_blob_client(container=\"stock-news-json\", blob=blob)\n",
    "      with open(blob, mode=\"wb\") as sample_blob:\n",
    "            download_stream = blob_client.download_blob()\n",
    "            sample_blob.write(download_stream.readall())\n",
    "\n",
    "all_data_dict = {}\n",
    "for json_file in blobs_to_use:\n",
    "      with open(json_file,\"r+\") as file:\n",
    "      # First we load existing data into a dict.\n",
    "            file_data = json.load(file)\n",
    "            all_data_dict.update(file_data)\n",
    "with open(\"merged_stock_news.json\", \"w\") as file:\n",
    "      file.write(json.dumps(all_data_dict, indent=4))\n",
    "\n",
    "# with open(args.blob_storage+\"/blobs_to_use.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(blob for blob in blobs_to_use), f)\n",
    "\n",
    "(Path(args.prep_output) / \"merged_stock_news.json\")#.write_text(\"\\n\".join(blob for blob in blobs_to_use))\n",
    "# continue here with this example -> https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/1a_pipeline_with_components_from_yaml/score_src/score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/classify.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--classify_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--classify_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# download distilbert model from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KernAI/stock-news-destilbert\")\n",
    "\n",
    "# retriev the list of blobs from the current day - input is a .txt file\n",
    "with open(args.classify_input, \"r\") as f:\n",
    "      data = f.read()\n",
    "texts = data[\"texts\"]\n",
    "\n",
    "sentiments = []\n",
    "for text in texts: \n",
    "      tokenized_text = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            is_split_into_words=False,\n",
    "            return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      outputs = model(tokenized_text[\"input_ids\"])\n",
    "      outputs_logits = outputs.logits.argmax(1)\n",
    "\n",
    "      mapping = {0: 'neutral', 1: 'negative', 2: 'positive'}\n",
    "      predicted_label = mapping[int(outputs_logits[0])]\n",
    "      sentiments.append(predicted_label)\n",
    "\n",
    "# add the sentiments to the data\n",
    "data[\"sentiments\"] = sentiments\n",
    "\n",
    "# overwrite old files with new files containing the sentiment\n",
    "with open(\"merged_stock_news.json\", \"w\") as f:\n",
    "      json.dump(data, f)\n",
    "\n",
    "(Path(args.prep_output) / \"merged_stock_news.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/summarize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/summarize.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--summarize_input\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "parser.add_argument(\"--summarize_output\", type=str, help=\"Mounted Azure ML blob storage\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the model and the tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\") \n",
    "\n",
    "# retriev the list of blobs from the current day - input is a .txt file\n",
    "with open(args.summarize_input, \"r\") as f:\n",
    "      data = f.read()\n",
    "texts = data[\"texts\"]\n",
    "\n",
    "summaries = []\n",
    "for text in texts: \n",
    "      # Tokenize our text\n",
    "      # If you want to run the code in Tensorflow, please remember to return the particular tensors as simply as using return_tensors = 'tf'\n",
    "      input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "      # Generate the output (Here, we use beam search but you can also use any other strategy you like)\n",
    "      output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=32, \n",
    "            num_beams=5, \n",
    "            early_stopping=True\n",
    "      )\n",
    "\n",
    "      # Finally, we can print the generated summary\n",
    "      summaries.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# add the sentiments to the data\n",
    "data[\"summaries\"] = summaries\n",
    "\n",
    "# overwrite old files with new files containing the sentiment\n",
    "with open(\"merged_stock_news.json\", \"w\") as f:\n",
    "      json.dump(data, f)\n",
    "\n",
    "(Path(args.prep_output) / \"merged_stock_news.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: stock-analysis-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - azure-storage-blob\n",
    "    - azure-identity\n",
    "    - azure-keyvault\n",
    "    - transformers\n",
    "    - sentencepiece\n",
    "    - numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env_name = \"stock-analysis-env\"\n",
    "version = \"1.5\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=version)\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock analysis pipeline\",\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/python-sdk-v2:4\",\n",
    "        version=version,\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = AssetTypes.URI_FOLDER\n",
    "mode = InputOutputModes.RW_MOUNT\n",
    "path = \"azureml://datastores/stocknewsjson/stock-news-json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Finding out which blobs to actually use\",\n",
    "    description=\"Loads files from Azure Blob Storage from todays \",\n",
    "    inputs={\n",
    "        \"blob_storage\": Input(mode=InputOutputModes.DIRECT)\n",
    "    },\n",
    "    outputs={\n",
    "        \"prep_output\": Output(type=data_type)\n",
    "    },\n",
    "    code=\"./components/prep.py\",\n",
    "    command=\"python prep.py --blob_storage ${{inputs.blob_storage}} --prep_output ${{outputs.prep_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Classify the sentiments of todays stock news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"classify_input\": Input(type=data_type), \n",
    "    },\n",
    "    outputs={\n",
    "        \"classify_output\": Output(type=data_type)\n",
    "    },\n",
    "    code=\"./components/classify.py\",\n",
    "    command=\"python classify.py --classify_input ${{inputs.classify_input}} --classify_output ${{outputs.classify_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Summarize the news\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\n",
    "        \"summarize_input\": Input(type=data_type),\n",
    "    },\n",
    "    outputs={\n",
    "        \"summarize_output\": Output(type=data_type, mode=InputOutputModes.RW_MOUNT, path=path)\n",
    "    },\n",
    "    code=\"./components/summarize.py\",\n",
    "    command=\"python summarize.py --summarize_input ${{inputs.summarize_input}} --summarize_output ${{outputs.summarize_output}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline(compute=\"ava\")\n",
    "def pipeline_with_python_function_components():\n",
    "\n",
    "    data_prep_job = data_prep_component(\n",
    "        blob_storage=\"https://mlstorageleo.blob.core.windows.net/\"\n",
    "    )\n",
    "    classify_job = classify_component(\n",
    "        classify_input=data_prep_job.outputs.prep_output\n",
    "\n",
    "    ) # feed putput of previous step into the training job\n",
    "    summarize_job = summarize_component(\n",
    "        summarize_input = classify_job.outputs.classify_output\n",
    "    )\n",
    "\n",
    "    return {\"processed_file\": summarize_job.outputs.summarize_output}\n",
    "\n",
    "pipeline_job = pipeline_with_python_function_components()\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"ava\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading prep.py\u001b[32m (< 1 MB): 100%|##########| 2.52k/2.52k [00:00<00:00, 48.9kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>stock-news-analysis-pipeline</td><td>olden_tent_vj2hjqwp1y</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/olden_tent_vj2hjqwp1y?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace&amp;tid=08548f02-0216-4325-938b-fd30f6829e55\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {'processed_file': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x0000017721F62640>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000017721F7A100>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'pipeline_with_python_function_components', 'is_deterministic': None, 'inputs': {}, 'outputs': {'processed_file': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'data_prep_job': Command({'parameters': {}, 'init': False, 'name': 'data_prep_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000017721F62430>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Finding out which blobs to actually use', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'blob_storage': {'type': 'uri_folder', 'path': 'https://mlstorageleo.blob.core.windows.net/'}}, 'job_outputs': {'prep_output': {'type': 'uri_folder'}}, 'inputs': {'blob_storage': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000017721F62400>}, 'outputs': {'prep_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x0000017721F624C0>}, 'component': 'azureml_anonymous:7bab20d4-85c1-4dd1-9bd8-555d256a718b', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '2a166f75-b72f-47b7-8605-8947f6940a6d', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'classify_job': Command({'parameters': {}, 'init': False, 'name': 'classify_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000017721F62760>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Classify the sentiments of todays stock news', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'classify_input': '${{parent.jobs.data_prep_job.outputs.prep_output}}'}, 'job_outputs': {'classify_output': {'type': 'uri_folder'}}, 'inputs': {'classify_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000017721F627C0>}, 'outputs': {'classify_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x0000017721F62730>}, 'component': 'azureml_anonymous:9b69b18a-753d-4464-93ee-3700ca4d43aa', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '367d821c-e1dc-4299-adba-2e39124dcdcb', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'summarize_job': Command({'parameters': {}, 'init': False, 'name': 'summarize_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000017721F626D0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Summarize the news', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'summarize_input': '${{parent.jobs.classify_job.outputs.classify_output}}'}, 'job_outputs': {'summarize_output': '${{parent.outputs.processed_file}}'}, 'inputs': {'summarize_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000017721F7A160>}, 'outputs': {'summarize_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x0000017721A2FF10>}, 'component': 'azureml_anonymous:fd02ba62-e349-453f-b173-0f550e598244', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'a14638ad-f7ec-4ea6-9aa0-7ed51a0d015e', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 3}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 3}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'olden_tent_vj2hjqwp1y', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'git@github.com:LeonardPuettmann/azure-stock-news-analysis.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '3d68bbf9b16bd0a91dfc1600e03d75043b57c928', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'ava', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/mlgroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace/jobs/olden_tent_vj2hjqwp1y', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\leopu\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-news-analysis\\\\news-analysis-pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000017721F62460>, 'serialize': <msrest.serialization.Serializer object at 0x0000017721F62880>, 'display_name': 'pipeline_with_python_function_components', 'experiment_name': 'stock-news-analysis-pipeline', 'compute': 'ava', 'services': {'Tracking': {'endpoint': 'azureml://northeurope.api.azureml.ms/mlflow/v1.0/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/mlgroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/olden_tent_vj2hjqwp1y?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace&tid=08548f02-0216-4325-938b-fd30f6829e55', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"stock-news-analysis-pipeline\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: olden_tent_vj2hjqwp1y\n",
      "Web View: https://ml.azure.com/runs/olden_tent_vj2hjqwp1y?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-07-08 13:36:34Z] Submitting 1 runs, first five are: f40962a7:cbfd23cb-9fab-4a1c-bb6c-8e00256d8577\n",
      "[2023-07-08 13:37:07Z] Completing processing run id cbfd23cb-9fab-4a1c-bb6c-8e00256d8577.\n",
      "[2023-07-08 13:37:07Z] Submitting 1 runs, first five are: 3bc4c20f:ea052e04-4e30-481b-affa-0773ec8f95f5\n",
      "[2023-07-08 13:37:35Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: olden_tent_vj2hjqwp1y\n",
      "Web View: https://ml.azure.com/runs/olden_tent_vj2hjqwp1y?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/mlgroup/workspaces/mlworkspace\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /classify_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"northeurope\",\n    \"location\": \"northeurope\",\n    \"time\": \"2023-07-08T13:37:35.246839Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Wait until the job completes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mstream(pipeline_job\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\leopu\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\leopu\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leopu\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:661\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[39mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[0;32m    659\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[39m=\u001b[39mjob_object\u001b[39m.\u001b[39mid)\n\u001b[1;32m--> 661\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_logs_until_completion(\n\u001b[0;32m    662\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runs_operations, job_object, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datastore_operations, requests_pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_pipeline\n\u001b[0;32m    663\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\leopu\\OneDrive\\Programming\\Python\\azure\\sdk-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:312\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    310\u001b[0m         file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[39mraise\u001b[39;00m JobException(\n\u001b[0;32m    313\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(json\u001b[39m.\u001b[39mdumps(error, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)),\n\u001b[0;32m    314\u001b[0m             target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mJOB,\n\u001b[0;32m    315\u001b[0m             no_personal_data_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException raised on failed job.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    316\u001b[0m             error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mSYSTEM_ERROR,\n\u001b[0;32m    317\u001b[0m         )\n\u001b[0;32m    319\u001b[0m file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m file_handle\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /classify_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"northeurope\",\n    \"location\": \"northeurope\",\n    \"time\": \"2023-07-08T13:37:35.246839Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
